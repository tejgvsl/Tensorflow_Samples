{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Train Video reading is Done!!!! ##########\n",
      "       Image_ID  Class\n",
      "0    frame0.jpg      1\n",
      "1    frame1.jpg      1\n",
      "2    frame2.jpg      2\n",
      "3    frame3.jpg      2\n",
      "4    frame4.jpg      2\n",
      "5    frame5.jpg      2\n",
      "6    frame6.jpg      2\n",
      "7    frame7.jpg      2\n",
      "8    frame8.jpg      2\n",
      "9    frame9.jpg      2\n",
      "10  frame10.jpg      2\n",
      "11  frame11.jpg      0\n",
      "12  frame12.jpg      2\n",
      "13  frame13.jpg      2\n",
      "14  frame14.jpg      1\n",
      "CSV reading is Done!!!! \n",
      "894\n",
      "(298, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 25,694,211\n",
      "Trainable params: 25,694,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 208 samples, validate on 90 samples\n",
      "Epoch 1/100\n",
      "208/208 [==============================] - 36s 172ms/step - loss: 1.0753 - acc: 0.3894 - val_loss: 0.9617 - val_acc: 0.3889\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 18s 85ms/step - loss: 0.8769 - acc: 0.4712 - val_loss: 0.9014 - val_acc: 0.4000\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 14s 68ms/step - loss: 0.7411 - acc: 0.4423 - val_loss: 0.7880 - val_acc: 0.4000\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 17s 84ms/step - loss: 0.6181 - acc: 0.4231 - val_loss: 0.7859 - val_acc: 0.4000\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 20s 98ms/step - loss: 0.5580 - acc: 0.4375 - val_loss: 0.7262 - val_acc: 0.4222\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 17s 84ms/step - loss: 0.5200 - acc: 0.4519 - val_loss: 0.7452 - val_acc: 0.4222\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 16s 77ms/step - loss: 0.4845 - acc: 0.4952 - val_loss: 0.6894 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 22s 106ms/step - loss: 0.3125 - acc: 0.8990 - val_loss: 0.6122 - val_acc: 0.7778\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 102s 488ms/step - loss: 0.1464 - acc: 0.9712 - val_loss: 0.5198 - val_acc: 0.8222\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 11s 55ms/step - loss: 0.0569 - acc: 0.9808 - val_loss: 0.4532 - val_acc: 0.8222\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 0.0505 - acc: 0.9904 - val_loss: 0.3871 - val_acc: 0.8444\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 7s 34ms/step - loss: 0.0527 - acc: 0.9760 - val_loss: 0.4342 - val_acc: 0.8222\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 16s 76ms/step - loss: 0.0330 - acc: 0.9856 - val_loss: 0.3658 - val_acc: 0.8556\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 9s 41ms/step - loss: 0.0238 - acc: 0.9904 - val_loss: 0.3654 - val_acc: 0.8333\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 9s 45ms/step - loss: 0.0205 - acc: 0.9904 - val_loss: 0.3510 - val_acc: 0.8556\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 6s 29ms/step - loss: 0.0141 - acc: 0.9904 - val_loss: 0.3444 - val_acc: 0.8444\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 8s 38ms/step - loss: 0.0137 - acc: 0.9904 - val_loss: 0.3447 - val_acc: 0.8444\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 0.0141 - acc: 0.9904 - val_loss: 0.3525 - val_acc: 0.8444\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 9s 42ms/step - loss: 0.0144 - acc: 0.9904 - val_loss: 0.3621 - val_acc: 0.8333\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 19s 90ms/step - loss: 0.0215 - acc: 0.9952 - val_loss: 0.3325 - val_acc: 0.8667\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 58s 281ms/step - loss: 0.0131 - acc: 0.9952 - val_loss: 0.4415 - val_acc: 0.8333\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 14s 66ms/step - loss: 0.0154 - acc: 0.9952 - val_loss: 0.3670 - val_acc: 0.8556\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 5s 22ms/step - loss: 0.0096 - acc: 0.9952 - val_loss: 0.3237 - val_acc: 0.8778\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 4s 22ms/step - loss: 0.0127 - acc: 0.9952 - val_loss: 0.3620 - val_acc: 0.8667\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 6s 27ms/step - loss: 0.0090 - acc: 0.9952 - val_loss: 0.3810 - val_acc: 0.8444\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 6s 29ms/step - loss: 0.0089 - acc: 0.9952 - val_loss: 0.3399 - val_acc: 0.8556\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 13s 61ms/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.3338 - val_acc: 0.8667\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 13s 61ms/step - loss: 0.0281 - acc: 0.9952 - val_loss: 0.5435 - val_acc: 0.8444\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 9s 41ms/step - loss: 0.0241 - acc: 0.9952 - val_loss: 0.3522 - val_acc: 0.8778\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 0.0187 - acc: 0.9952 - val_loss: 0.3769 - val_acc: 0.8444\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 12s 55ms/step - loss: 0.0209 - acc: 0.9952 - val_loss: 0.4417 - val_acc: 0.8444\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 29s 138ms/step - loss: 0.0280 - acc: 0.9952 - val_loss: 0.3342 - val_acc: 0.8889\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 7s 34ms/step - loss: 0.0280 - acc: 0.9904 - val_loss: 0.6388 - val_acc: 0.8333\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 0.0234 - acc: 0.9952 - val_loss: 0.3940 - val_acc: 0.8444\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.3530 - val_acc: 0.8667\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 15s 70ms/step - loss: 0.0126 - acc: 0.9904 - val_loss: 0.3583 - val_acc: 0.8556\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 20s 95ms/step - loss: 0.0127 - acc: 0.9904 - val_loss: 0.3542 - val_acc: 0.8556\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 8s 39ms/step - loss: 0.0084 - acc: 0.9952 - val_loss: 0.4652 - val_acc: 0.8444\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 17s 84ms/step - loss: 0.0126 - acc: 0.9904 - val_loss: 0.3993 - val_acc: 0.8556\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 16s 77ms/step - loss: 0.0089 - acc: 0.9952 - val_loss: 0.3822 - val_acc: 0.8556\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 7s 35ms/step - loss: 0.0076 - acc: 0.9952 - val_loss: 0.3676 - val_acc: 0.8667\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 18s 88ms/step - loss: 0.0118 - acc: 0.9952 - val_loss: 0.3542 - val_acc: 0.8556\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 13s 60ms/step - loss: 0.0079 - acc: 0.9952 - val_loss: 0.4102 - val_acc: 0.8556\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 11s 51ms/step - loss: 0.0179 - acc: 0.9952 - val_loss: 0.4675 - val_acc: 0.8444\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 10s 47ms/step - loss: 0.0099 - acc: 0.9952 - val_loss: 0.3417 - val_acc: 0.8889\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 0.0186 - acc: 0.9952 - val_loss: 0.3400 - val_acc: 0.8778\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 6s 28ms/step - loss: 0.0118 - acc: 0.9904 - val_loss: 0.5910 - val_acc: 0.8333\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 14s 66ms/step - loss: 0.0205 - acc: 0.9952 - val_loss: 0.4576 - val_acc: 0.8444\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 6s 27ms/step - loss: 0.0216 - acc: 0.9904 - val_loss: 0.3611 - val_acc: 0.8889\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 6s 27ms/step - loss: 0.0150 - acc: 0.9904 - val_loss: 0.3916 - val_acc: 0.8556\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 21s 99ms/step - loss: 0.0087 - acc: 0.9952 - val_loss: 0.3877 - val_acc: 0.8556\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 20s 95ms/step - loss: 0.0107 - acc: 0.9904 - val_loss: 0.3778 - val_acc: 0.8556\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 12s 60ms/step - loss: 0.0085 - acc: 0.9952 - val_loss: 0.3997 - val_acc: 0.8556\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 12s 58ms/step - loss: 0.0111 - acc: 0.9952 - val_loss: 0.4557 - val_acc: 0.8444\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 8s 39ms/step - loss: 0.0083 - acc: 0.9952 - val_loss: 0.3902 - val_acc: 0.8556\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 16s 76ms/step - loss: 0.0137 - acc: 0.9952 - val_loss: 0.3464 - val_acc: 0.8889\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 10s 47ms/step - loss: 0.0113 - acc: 0.9952 - val_loss: 0.3749 - val_acc: 0.8556\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 5s 26ms/step - loss: 0.0071 - acc: 0.9952 - val_loss: 0.4613 - val_acc: 0.8444\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 6s 29ms/step - loss: 0.0087 - acc: 0.9952 - val_loss: 0.4464 - val_acc: 0.8444\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 5s 26ms/step - loss: 0.0091 - acc: 0.9952 - val_loss: 0.4400 - val_acc: 0.8556\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 5s 25ms/step - loss: 0.0080 - acc: 0.9952 - val_loss: 0.3898 - val_acc: 0.8556\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 5s 24ms/step - loss: 0.0099 - acc: 0.9952 - val_loss: 0.3628 - val_acc: 0.8556\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 5s 24ms/step - loss: 0.0087 - acc: 0.9952 - val_loss: 0.3925 - val_acc: 0.8556\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 6s 27ms/step - loss: 0.0080 - acc: 0.9952 - val_loss: 0.4318 - val_acc: 0.8556\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 5s 22ms/step - loss: 0.0077 - acc: 0.9952 - val_loss: 0.4034 - val_acc: 0.8556\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 4s 21ms/step - loss: 0.0098 - acc: 0.9952 - val_loss: 0.3773 - val_acc: 0.8667\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 5s 25ms/step - loss: 0.0097 - acc: 0.9904 - val_loss: 0.4399 - val_acc: 0.8556\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 5s 25ms/step - loss: 0.0097 - acc: 0.9952 - val_loss: 0.4206 - val_acc: 0.8667\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 5s 24ms/step - loss: 0.0082 - acc: 0.9952 - val_loss: 0.3809 - val_acc: 0.8667\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 7s 35ms/step - loss: 0.0087 - acc: 0.9904 - val_loss: 0.4039 - val_acc: 0.8556\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 8s 40ms/step - loss: 0.0085 - acc: 0.9904 - val_loss: 0.3980 - val_acc: 0.8556\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 8s 38ms/step - loss: 0.0086 - acc: 0.9952 - val_loss: 0.3970 - val_acc: 0.8556\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 16s 78ms/step - loss: 0.0077 - acc: 0.9952 - val_loss: 0.4308 - val_acc: 0.8667\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 6s 29ms/step - loss: 0.0111 - acc: 0.9952 - val_loss: 0.4892 - val_acc: 0.8444\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 0.0104 - acc: 0.9952 - val_loss: 0.3867 - val_acc: 0.8556\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 5s 26ms/step - loss: 0.0084 - acc: 0.9952 - val_loss: 0.3929 - val_acc: 0.8556\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 5s 25ms/step - loss: 0.0085 - acc: 0.9904 - val_loss: 0.4086 - val_acc: 0.8556\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 5s 23ms/step - loss: 0.0076 - acc: 0.9952 - val_loss: 0.3817 - val_acc: 0.8444\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 5s 24ms/step - loss: 0.0083 - acc: 0.9952 - val_loss: 0.4025 - val_acc: 0.8556\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 5s 24ms/step - loss: 0.0080 - acc: 0.9952 - val_loss: 0.4073 - val_acc: 0.8556\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 5s 23ms/step - loss: 0.0074 - acc: 0.9952 - val_loss: 0.4275 - val_acc: 0.8667\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 7s 33ms/step - loss: 0.0076 - acc: 0.9952 - val_loss: 0.4452 - val_acc: 0.8556\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 8s 37ms/step - loss: 0.0080 - acc: 0.9952 - val_loss: 0.3979 - val_acc: 0.8556\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 7s 33ms/step - loss: 0.0071 - acc: 0.9952 - val_loss: 0.4234 - val_acc: 0.8556\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 5s 24ms/step - loss: 0.0073 - acc: 0.9952 - val_loss: 0.4458 - val_acc: 0.8556\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 41s 199ms/step - loss: 0.0073 - acc: 0.9952 - val_loss: 0.4370 - val_acc: 0.8667\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 15s 70ms/step - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4244 - val_acc: 0.8667\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4191 - val_acc: 0.8667\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 13s 61ms/step - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4077 - val_acc: 0.8556\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 15s 71ms/step - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4024 - val_acc: 0.8667\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 10s 48ms/step - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4030 - val_acc: 0.8667\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 8s 38ms/step - loss: 0.0072 - acc: 0.9904 - val_loss: 0.4046 - val_acc: 0.8667\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 20s 96ms/step - loss: 0.0071 - acc: 0.9952 - val_loss: 0.4111 - val_acc: 0.8556\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 16s 76ms/step - loss: 0.0073 - acc: 0.9952 - val_loss: 0.4241 - val_acc: 0.8667\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 11s 55ms/step - loss: 0.0074 - acc: 0.9904 - val_loss: 0.3974 - val_acc: 0.8667\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 26s 127ms/step - loss: 0.0071 - acc: 0.9952 - val_loss: 0.3958 - val_acc: 0.8667\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 9s 45ms/step - loss: 0.0073 - acc: 0.9952 - val_loss: 0.3873 - val_acc: 0.8667\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 9s 44ms/step - loss: 0.0070 - acc: 0.9952 - val_loss: 0.4042 - val_acc: 0.8556\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 11s 54ms/step - loss: 0.0070 - acc: 0.9952 - val_loss: 0.4170 - val_acc: 0.8556\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 0.0071 - acc: 0.9952 - val_loss: 0.4320 - val_acc: 0.8667\n",
      "##########  Trainig is Done  ############\n",
      "********* Reading test video is Done! ********\n",
      "********* Reading test CSV is Done! ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "import cv2     # for capturing videos\n",
    "import math   # for mathematical operations\n",
    "import os\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "from skimage.transform import resize   # for resizing images\n",
    "import sklearn\n",
    "Test_path = './Image_frames/Test_frames'\n",
    "Train_path = './Image_frames/Train_frames'\n",
    "count = 0\n",
    "videoFile = \"Tom and jerry.mp4\"\n",
    "#cap = cv2.VideoCapture(videoFile)   # capturing the video from the given path\n",
    "#frameRate = cap.get(cv2.CAP_PROP_FPS) #frame rate\n",
    "#print('Frame rate = ' , frameRate)\n",
    "\n",
    "'''while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(os.path.join(Train_path , filename), frame)\n",
    "cap.release()'''\n",
    "print (\"############## Train Video reading is Done!!!! ##########\")\n",
    "\n",
    "#img = cv2.imread('./Image_frames/frame0.jpg')   # reading image using its name\n",
    "#cv2.imshow('Frame',img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "data = pd.read_csv('mapping.csv')     # reading the csv file\n",
    "print(data.head(15))      # printing first five rows of the file\n",
    "print (\"CSV reading is Done!!!! \")\n",
    "\n",
    "#Our next step is to read the images which we will do based on their names, aka, the Image_ID column.\n",
    "X = [ ]     # creating an empty array\n",
    "for img_name in data.Image_ID:\n",
    "    img = plt.imread(os.path.join(Train_path , img_name))\n",
    "    X.append(img)  # storing each image in array X\n",
    "X = np.array(X)    # converting list to array\n",
    "\n",
    "#Since there are three classes, we will one hot encode them using the to_categorical() function of keras.utils.\n",
    "y = data.Class\n",
    "dummy_y = np_utils.to_categorical(y)    # one hot encoding Classes\n",
    "print(dummy_y.size)\n",
    "print(dummy_y.shape)\n",
    "\n",
    "#We will be using a VGG16 pretrained model which takes an input image of shape (224 X 224 X 3).\n",
    "#Since our images are in a different size, we need to reshape all of them.\n",
    "#We will use the resize() function of skimage.transform to do this.\n",
    "image = [ ]\n",
    "for i in range(0,X.shape[0]):\n",
    "    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\n",
    "    image.append(a)\n",
    "image = np.array(image)\n",
    "\n",
    "#All the images have been reshaped to 224 X 224 X 3. But before passing any input to the model,\n",
    "#we must preprocess it as per the model’s requirement. Otherwise, the model will not perform well enough.\n",
    "#Use the preprocess_input() function of keras.applications.vgg16 to perform this step.\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "X = preprocess_input(image, mode='tf')      # preprocessing the input data\n",
    "\n",
    "#We also need a validation set to check the performance of the model on unseen images.\n",
    "#We will make use of the train_test_split() function of the sklearn.model_selection module to randomly\n",
    "#divide images into training and validation set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)    # preparing the validation set\n",
    "\n",
    "#The next step is to build our model. As mentioned, we shall be using the VGG16 pretrained model for this task.\n",
    "#Let us first import the required libraries to build the model:\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout\n",
    "\n",
    "#We will now load the VGG16 pretrained model and store it as base_model:\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer\n",
    "\n",
    "#We will make predictions using this model for X_train and X_valid, get the features,\n",
    "#and then use those features to retrain the model.\n",
    "X_train = base_model.predict(X_train)\n",
    "X_valid = base_model.predict(X_valid)\n",
    "X_train.shape, X_valid.shape\n",
    "\n",
    "#The shape of X_train and X_valid is (208, 7, 7, 512), (90, 7, 7, 512) respectively.\n",
    "#In order to pass it to our neural network, we have to reshape it to 1-D.\n",
    "X_train = X_train.reshape(208, 7*7*512)      # converting to 1-D\n",
    "X_valid = X_valid.reshape(90, 7*7*512)\n",
    "\n",
    "#We will now preprocess the images and make them zero-centered which helps the model to converge faster.\n",
    "train = X_train/X_train.max()      # centering the data\n",
    "X_valid = X_valid/X_train.max()\n",
    "\n",
    "#Finally, we will build our model. This step can be divided into 3 sub-steps:\n",
    "#1. Building the model\n",
    "#2. Compiling the model\n",
    "#3. Training the model\n",
    "\n",
    "#Building the model\n",
    "model = Sequential()\n",
    "model.add(InputLayer((7*7*512,)))    # input layer\n",
    "model.add(Dense(units=1024, activation='sigmoid')) # hidden layer\n",
    "model.add(Dense(3, activation='sigmoid'))    # output layer\n",
    "\n",
    "#printing Model summary\n",
    "model.summary()\n",
    "\n",
    "#We have a hidden layer with 1,024 neurons and an output layer with 3 neurons (since we have 3 classes to predict).\n",
    "#Now we will compile our model:\n",
    "#Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train, y_train, epochs=100, validation_data=(X_valid, y_valid))\n",
    "\n",
    "print(\"##########  Trainig is Done  ############\")\n",
    "\n",
    "#Calculating the screen time – A simple solution\n",
    "#First, download the video we’ll be using in this section from here.\n",
    "#Once done, go ahead and load the video and extract frames from it.\n",
    "#We will follow the same steps as we did above:\n",
    "count = 0\n",
    "videoFile = \"Tom and Jerry 3.mp4\"\n",
    "#cap = cv2.VideoCapture(videoFile)\n",
    "#frameRate = cap.get(cv2.CAP_PROP_FPS) #frame rate\n",
    "x=1\n",
    "'''while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"test%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(os.path.join(Test_path , filename), frame)\n",
    "cap.release()'''\n",
    "print (\"********* Reading test video is Done! ********\")\n",
    "\n",
    "#After extracting the frames from the new video,\n",
    "#we will now load the test.csv file which contains the names of each extracted frame.\n",
    "#Download the test.csv file and load it:\n",
    "test = pd.read_csv('test.csv')\n",
    "print (\"********* Reading test CSV is Done! ********\")\n",
    "#Next, we will import the images for testing and then reshape them\n",
    "#as per the requirements of the aforementioned pretrained model:\n",
    "test_input = [ ]\n",
    "for img_name in test.Image_ID:\n",
    "    img = plt.imread(os.path.join(Test_path , img_name)) #plt.imread('' + img_name)\n",
    "    test_input.append(img)\n",
    "test_input = np.array(test_input)\n",
    "\n",
    "test_image = [ ]\n",
    "for i in range(0,test_input.shape[0]):\n",
    "    a = resize(test_input[i], preserve_range=True, output_shape=(224,224)).astype(int)\n",
    "    test_image.append(a)\n",
    "test_image = np.array(test_image)\n",
    "\n",
    "#We need to make changes to these images similar to the ones\n",
    "#we did for the training images. We will preprocess the images,\n",
    "#use the base_model.predict() function to extract features from these images\n",
    "#using the VGG16 pretrained model, reshape these images to 1-D form, and make them zero-centered:\n",
    "\n",
    "# preprocessing the images\n",
    "test_image = preprocess_input(test_image, mode='tf')\n",
    "\n",
    "# extracting features from the images using pretrained model\n",
    "test_image = base_model.predict(test_image)\n",
    "\n",
    "# converting the images to 1-D form\n",
    "test_image = test_image.reshape(186, 7*7*512)\n",
    "\n",
    "# zero centered images\n",
    "test_image = test_image/test_image.max()\n",
    "\n",
    "#Since we have trained the model previously, we will make use of that model to make prediction for these images\n",
    "predictions = model.predict_classes(test_image)\n",
    "\n",
    "#Calculate the screen time of both TOM and JERRY\n",
    "#Recall that Class ‘1’ represents the presence of JERRY, while Class ‘2’ represents the presence of TOM.\n",
    "#We shall make use of the above predictions to calculate the screen time of both these legendary characters:\n",
    "\n",
    "print(\"The screen time of JERRY is\", predictions[predictions==1].shape[0], \"seconds\")\n",
    "print(\"The screen time of TOM is\", predictions[predictions==2].shape[0], \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
